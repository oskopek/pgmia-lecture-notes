\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
%\usepackage{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
\usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\newcommand{\E}[0]{\mathbb{E}}

\title{Lecture Notes and Topic Summaries\\{\normalfont\large \textsc{Probabilistic Graphical Models for Image Analysis}}}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Ondrej Skopek\\
  Department of Computer Science\\
  ETH Z\"{u}rich\\
  \texttt{oskopek@ethz.ch}\\
  \And
  Lukas Jendele\\
  Department of Computer Science\\
  ETH Z\"{u}rich\\
  \texttt{jendelel@ethz.ch}\\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}

\maketitle

\section{Lecture notes}

\subsection{Lecture 1 --- Introduction to Graphical Models --- 2018/09/21}

Test

\subsection{Lecture 2 --- Variational Inference --- 2018/09/28}

\subsection{Lecture 3 --- Expectation Propagation --- 2018/10/05}

\subsection{Lecture 4 --- Repetition and Stochastic Variational Inference --- 2018/10/12}

\subsection{Lecture 5 --- Sequential Data --- 2018/10/19}

\subsection{Lecture 6 --- Dimensionality Reduction --- 2018/10/26}

\subsection{Lecture 7 --- Summary Dimensionality Reduction and State Space Models --- 2018/11/02}

\subsection{Lecture 8 --- Guest Lecture: Generative Adversarial Networks --- 2018/11/09}

Google talk. N/A for exam?

\subsection{Lecture 9 --- Autoencoding Variational Bayes --- 2018/11/16}

\subsection{Lecture 10 --- Score Function Estimators --- 2018/11/23}

\subsection{Lecture 11 --- Evaluating Deep Representation Learning --- 2018/11/30}

\subsection{Lecture 12 --- Guest lecture: Temporal Point Processes and Bayesian Non-parametrics --- 2018/12/14}

N/A for exam.

\subsection{Lecture 13 --- Guest lecture --- 2018/12/21}

N/A for exam.







\section{Topics}



\subsection{EM Algorithm}

\subsubsection{Concept and examples e.g. EM for Gaussian Mixture Model}


\subsubsection{Why does the algorithm converge?}









\subsection{Variational Inference}


\subsubsection{Concept and examples e.g.~Mean-field for Gaussian Mixture with Dirichlet Prior}

\subsubsection{Expectation Propagation and $\alpha$ divergence}


\subsubsection{Examples e.g. Expectation Propagation for Gaussian Mixture}


\subsubsection{Explain exponential families and what they offer, especially wrt. Variational Inference}


\subsubsection{Explain the key concepts of MCMC (e.g. detailed balance, normalization) and the advantages disadvantages of MCMC vs Variational Inference}









\subsection{Extensions of Variational Inference}

\subsubsection{Explain SVI and its motivation and problems}

\subsubsection{What is a natural gradient and the motivation for it?}

\subsubsection{Explain Black Box Variational Inference}

\subsubsection{What are the disadvantages of Black Box Var. Inference?}

\subsubsection{Show variance reduction through control variates. What is one typical and simple choice for a control variate?}






\subsection{Dimensionality Reduction}

\subsubsection{Derive Factor Models, PCA, Probabilistic PCA}

\subsubsection{Derive EM for Factor Models}

\subsubsection{Show the relation between variance and reconstruction error}

\subsubsection{Show the relation between SVD and PCA}

\subsubsection{Explain the difference/similarities between PCA and linear least squares}

\subsubsection{Explain and derive the Eigenface Algorithm, name problems and results wrt. implementation and performance}

\subsubsection{Explain the Kernel Trick and derive Kernel PCA}

\subsubsection{What would be advantages and disadvantages wrt.~the Eigenface Algorithm when using the Kernel trick?}







\subsection{Sequential Data}

\subsubsection{Explain HMM and Kalman Filter, especially Inference and Learning}

\subsubsection{Explain the difference between the Kalman Filter and the Rauch Tung Striebel smoother.}

\subsection{Score Functions}

\subsubsection{Explain the concept and Score Function Estimators}

\subsubsection{Explain the log-derivative trick and how to derive:
$\nabla_\theta \E_{x \sim p(x|\theta}[f(x)] = \E_x[f(x) \nabla_\theta \log p(x|\theta)]$}

\subsubsection{Relate the algorithm to policy gradients and discuss the problems of the algorithm}







\subsection{Variational Autoencoders}

\subsubsection{Explain the concept in detail and the motivation for distributional assumptions.}

\subsubsection{Explain the reparameterization trick.}

\subsubsection{Explain the main ideas and properties of SGD. How is it useful in the context of VAE and why do we need the reparameterization trick?}

\subsubsection{Show the derivation of VAE's from black box variational inference.}

\subsubsection{Explain the cost function of beta-VAE and outline 3 desired effects of the cost function compared to a classic VAE.}




\subsection{Deep Generative Models}

\subsubsection{Explain the cost function of GANs what are advantages and disadvantages of GANs compared to VAEs?}

\subsubsection{Explain the difficulty of evaluating generative models and especially their latent representation.}

\subsubsection{Explain ICA and how it relates to recent approaches for learning ``disentangled'' representations}




%\bibliography{references.bib}
%\bibliographystyle{}

\end{document}
